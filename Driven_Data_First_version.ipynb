{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A records (8203, 345)\n"
     ]
    }
   ],
   "source": [
    "input_df1 = pd.read_csv('C://Users//saish//Documents//driven_data//A_hhold_train.csv', index_col='id')\n",
    "print('A records', input_df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B records (3255, 442)\n"
     ]
    }
   ],
   "source": [
    "input_df2 = pd.read_csv('C://Users//saish//Documents//driven_data//B_hhold_train.csv', index_col='id')\n",
    "print('B records', input_df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C records (6469, 164)\n"
     ]
    }
   ],
   "source": [
    "input_df3 = pd.read_csv('C://Users//saish//Documents//driven_data//C_hhold_train.csv', index_col='id')\n",
    "print('C records', input_df3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data imbalance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    4500\n",
       "True     3703\n",
       "Name: poor, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df1.poor.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3004\n",
       "True      251\n",
       "Name: poor, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df2.poor.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5496\n",
       "True      973\n",
       "Name: poor, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df3.poor.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preprocess(input_df1, enforce_cols=None):\n",
    "    print('Initial Input Shape', input_df1.shape)\n",
    "    numeric = input_df1.select_dtypes(include=['int64', 'float64'])\n",
    "    input_df1[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    print('After standardization Input Shape', input_df1.shape)\n",
    "    input_df1 = pd.get_dummies(input_df1)\n",
    "    print('After encoding Input Shape', input_df1.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    processing for test set\n",
    "    setdiffid(a,b) = give values that are in 'a' but not 'b'\n",
    "    \"\"\"\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(input_df1.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, input_df1.columns)\n",
    "        \n",
    "        input_df1.drop(to_drop, axis=1, inplace=True)\n",
    "        input_df1 = input_df1.assign(**{c: 0 for c in to_add})\n",
    "        \n",
    "    return input_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Input Shape (8203, 344)\n",
      "After standardization Input Shape (8203, 344)\n",
      "After encoding Input Shape (8203, 859)\n",
      "Initial Input Shape (3255, 441)\n",
      "After standardization Input Shape (3255, 441)\n",
      "After encoding Input Shape (3255, 1432)\n",
      "Initial Input Shape (6469, 163)\n",
      "After standardization Input Shape (6469, 163)\n",
      "After encoding Input Shape (6469, 795)\n"
     ]
    }
   ],
   "source": [
    "aX_train = data_preprocess(input_df1.drop('poor', axis=1))\n",
    "ay_train = np.ravel(input_df1.poor)\n",
    "bX_train = data_preprocess(input_df2.drop('poor', axis=1))\n",
    "by_train = np.ravel(input_df2.poor)\n",
    "cX_train = data_preprocess(input_df3.drop('poor', axis=1))\n",
    "cy_train = np.ravel(input_df3.poor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input 1 after pre-processing (8203, 859)\n",
      "shape of input 2 after pre-processing (3255, 1432)\n",
      "shape of input 3 after pre-processing (6469, 795)\n"
     ]
    }
   ],
   "source": [
    "print('shape of input 1 after pre-processing', aX_train.shape)\n",
    "print('shape of input 2 after pre-processing', bX_train.shape)\n",
    "print('shape of input 3 after pre-processing', cX_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bX_train = bX_train.fillna(bX_train.mean())\n",
    "#bX_train.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cX_train.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "b_in, b_out = SMOTE().fit_sample(np.asarray(bX_train), np.asarray(by_train))\n",
    "c_in, c_out = SMOTE().fit_sample(np.asarray(cX_train), np.asarray(cy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import Counter \n",
    "print(sorted(Counter(b_out).items()))\n",
    "print(sorted(Counter(c_out).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DownSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn.utils import resample\n",
    "b_in, b_out = resample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A records (4041, 344)\n",
      "B records (1604, 441)\n",
      "C records (3187, 163)\n",
      "Initial Input Shape (4041, 344)\n",
      "After standardization Input Shape (4041, 344)\n",
      "After encoding Input Shape (4041, 851)\n",
      "Initial Input Shape (1604, 441)\n",
      "After standardization Input Shape (1604, 441)\n",
      "After encoding Input Shape (1604, 1419)\n",
      "Initial Input Shape (3187, 163)\n",
      "After standardization Input Shape (3187, 163)\n",
      "After encoding Input Shape (3187, 773)\n"
     ]
    }
   ],
   "source": [
    "test_df1 = pd.read_csv('C://Users//saish//Documents//driven_data//A_hhold_test.csv', index_col='id')\n",
    "print('A records', test_df1.shape)\n",
    "test_df2 = pd.read_csv('C://Users//saish//Documents//driven_data//B_hhold_test.csv', index_col='id')\n",
    "print('B records', test_df2.shape)\n",
    "test_df3 = pd.read_csv('C://Users//saish//Documents//driven_data//C_hhold_test.csv', index_col='id')\n",
    "print('C records', test_df3.shape)\n",
    "##\n",
    "a_test = data_preprocess(test_df1, enforce_cols=aX_train.columns)\n",
    "##\n",
    "b_test = data_preprocess(test_df2, enforce_cols=bX_train.columns)\n",
    "b_test = b_test.fillna(b_test.mean())\n",
    "#b_test.isnull().sum().sort_values(ascending=False)\n",
    "##\n",
    "c_test = data_preprocess(test_df3, enforce_cols=cX_train.columns)\n",
    "c_test = c_test.fillna(c_test.mean())\n",
    "#c_test.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Country A\n",
    "a_svd = TruncatedSVD(algorithm='randomized', n_components=500, n_iter=10, random_state=42)\n",
    "a_tr_svd = a_svd.fit_transform(aX_train, ay_train)\n",
    "a_ts_svd = a_svd.transform(a_test)\n",
    "print('A - country variance',a_svd.explained_variance_ratio_.sum()) \n",
    "print('A - Input Dimension',a_tr_svd.shape)\n",
    "\n",
    "#Country B\n",
    "b_svd = TruncatedSVD(algorithm='randomized', n_components=1000, n_iter=10, random_state=42)\n",
    "b_tr_svd = b_svd.fit_transform(bX_train, by_train)\n",
    "b_ts_svd = b_svd.transform(b_test)\n",
    "print('B - country variance',b_svd.explained_variance_ratio_.sum()) \n",
    "print('B - Input Dimension',b_tr_svd.shape)\n",
    "\n",
    "#Country C\n",
    "c_svd = TruncatedSVD(algorithm='randomized', n_components=600, n_iter=10, random_state=42)\n",
    "c_tr_svd = c_svd.fit_transform(cX_train, cy_train)\n",
    "c_ts_svd = c_svd.transform(c_test)\n",
    "print('C - country variance',c_svd.explained_variance_ratio_.sum()) \n",
    "print('C - Input Dimension',c_tr_svd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_x, ts_x, tr_y, ts_y = train_test_split(a_tr_svd, ay_train, test_size = 0.2, random_state=42)\n",
    "#model_a = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "#model_a = LogisticRegression()\n",
    "model_a = AdaBoostClassifier(n_estimators= 500)\n",
    "#model_a = GradientBoostingClassifier()\n",
    "#model_a = tree.DecisionTreeClassifier()\n",
    "#model_a = GaussianNB()\n",
    "#model_a = ExtraTreesClassifier()\n",
    "model_a.fit(tr_x, tr_y)\n",
    "tr_pred_a = model_a.predict(ts_x)\n",
    "#\n",
    "accuracy = model_a.score(ts_x, ts_y)\n",
    "print(\"In-sample accuracy:\",accuracy)\n",
    "#classification report\n",
    "print(classification_report(ts_y,tr_pred_a))\n",
    "#\n",
    "a_pred = model_a.predict_proba(a_ts_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bX_train_int, by_train_int = shuffle(b_tr_svd, by_train, random_state=0)\n",
    "tr_x, ts_x, tr_y, ts_y = train_test_split(b_tr_svd, by_train, test_size = 0.2, random_state=42)\n",
    "#model_b = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "#model_b = LogisticRegression()\n",
    "model_b = AdaBoostClassifier(n_estimators= 500)\n",
    "#model_b = GradientBoostingClassifier()\n",
    "#model_b = GradientBoostingClassifier()\n",
    "#model_b = tree.DecisionTreeClassifier()\n",
    "#model_b = GaussianNB()\n",
    "#model_b = ExtraTreesClassifier()\n",
    "model_b.fit(tr_x, tr_y)\n",
    "tr_pred_b = model_b.predict(ts_x)\n",
    "#\n",
    "accuracy = model_b.score(ts_x, ts_y)\n",
    "print(\"In-sample accuracy:\",accuracy)\n",
    "#classification report\n",
    "print(classification_report(ts_y,tr_pred_b))\n",
    "#\n",
    "b_pred = model_b.predict_proba(b_ts_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cX_train_int, cy_train_int = shuffle(c_in, c_out, random_state=0)\n",
    "tr_x, ts_x, tr_y, ts_y = train_test_split(c_tr_svd, cy_train, test_size = 0.2, random_state=42)\n",
    "#model_c = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "#model_c = LogisticRegression()\n",
    "model_c = AdaBoostClassifier(n_estimators= 900)\n",
    "#model_c = GradientBoostingClassifier()\n",
    "#model_c = tree.DecisionTreeClassifier()\n",
    "#model_c = GaussianNB()\n",
    "model_c = ExtraTreesClassifier()\n",
    "model_c.fit(tr_x, tr_y)\n",
    "tr_pred_c = model_c.predict(ts_x)\n",
    "#\n",
    "accuracy = model_c.score(ts_x, ts_y)\n",
    "print(\"In-sample accuracy:\",accuracy)\n",
    "#classification report\n",
    "print(classification_report(ts_y,tr_pred_c))\n",
    "#\n",
    "c_pred = model_c.predict_proba(c_ts_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change prediction format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_country_sub(preds, test_feat, country):\n",
    "    # make sure we code the country correctly\n",
    "    country_codes = ['A', 'B', 'C']\n",
    "    \n",
    "    # get just the poor probabilities\n",
    "    country_sub = pd.DataFrame(data=preds[:, 1],  # proba p=1\n",
    "                               columns=['poor'], \n",
    "                               index=test_feat.index)\n",
    "\n",
    "    \n",
    "    # add the country code for joining later\n",
    "    country_sub[\"country\"] = country\n",
    "    return country_sub[[\"country\", \"poor\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_sub = make_country_sub(a_pred, a_test, 'A')\n",
    "b_sub = make_country_sub(b_pred, b_test, 'B')\n",
    "c_sub = make_country_sub(c_pred, c_test, 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_fl = pd.concat([a_sub, b_sub, c_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_fl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_fl.to_csv('submission_ab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
